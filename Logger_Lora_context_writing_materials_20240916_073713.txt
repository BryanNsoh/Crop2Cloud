Context extraction timestamp: 20240916_073713

<repository_structure>
<directory name="writing_materials">
    <file>
        <name>cloud functions explained.txt</name>
        <path>cloud functions explained.txt</path>
        <content>
You're absolutely right! We need to go full "research paper mode" and leave no stone unturned. Here's a breakdown of the Cloud Functions with maximum detail, including formulas and avoiding code reproduction:

## Cloud Functions: An Exhaustive Examination

**1. Data Ingestion Function:**

* **Trigger:** Google Cloud Pub/Sub message arrival on the `tester` topic. Each message originates from the MQTT forwarder running on the dedicated Google Cloud VM.
* **Purpose:** 
    * **Sensor Data Digestion:** Efficiently parse incoming Pub/Sub messages, extracting timestamped sensor readings and associated metadata.
    * **Hash to Metadata Resolution:** Crucially, map the compact three-digit sensor hashes contained within the messages to their corresponding full sensor metadata (using `sensor_mapping.yaml`). This ensures data fidelity and facilitates targeted storage in BigQuery.
* **Inputs:** 
    * **Pub/Sub Message:**  A JSON payload containing the following:
        * `timestamp`: The timestamp of the sensor measurement, as provided by the Raspberry Pi node.
        * `sensor_id`: The three-digit sensor hash.
        * `value`: The numerical sensor reading.
        * `project_name`: The target Google Cloud project ID ("crop2cloud24").
        * `dataset_name`: The BigQuery dataset to be used, formatted as "{field}_trt{treatment}" (e.g., "LINEAR_CORN_trt1").
        * `table_name`:  The BigQuery table for data storage, formatted as "plot_{plot_number}" (e.g., "plot_5023").
    * **`sensor_mapping.yaml`:**  This YAML file, deployed with the Cloud Function, holds the critical mapping between sensor hashes and their full metadata. The structure is a list of dictionaries, with each dictionary representing a sensor.  A dictionary might look like this:

        ```yaml
        - hash: "035"
          treatment: 1
          plot_number: 5023
          project_id: crop2cloud24
          dataset_id: node_a
          sensor_id: TDR5023A10624 
          span: 5
          sdi-12_address: "2"
          depth: 6
          node: A
          field: LINEAR_CORN
        ```
* **Process:**
    1. **Message Acquisition and Parsing:**  The function receives the Pub/Sub message trigger and decodes the JSON payload into a Python dictionary.
    2. **Hash-to-Metadata Lookup:** 
        * The function opens and parses the `sensor_mapping.yaml` file using the `yaml` library.
        * It iterates through the list of sensor entries (dictionaries) in the `sensor_mapping.yaml` data. 
        * For each entry, it compares the `hash` field to the `sensor_id` value from the Pub/Sub message.
        * When a match is found, it extracts the full `sensor_id`, `field`, `treatment`, and `plot_number` from the matching YAML entry. This information is now ready to be used for precise data storage in BigQuery.
    3. **BigQuery Row Construction:** The function dynamically creates a dictionary representing the row to be inserted into BigQuery. The dictionary keys correspond to the column names in the BigQuery table. This dictionary will contain:
        * `TIMESTAMP`: The timestamp from the Pub/Sub message.
        * The full sensor ID (retrieved from the metadata mapping). 
        * `value`: The sensor reading from the message.
        * `is_actual`: A boolean value, likely set to `True` to indicate that this is actual sensor data (as opposed to predicted or interpolated values).
    4. **BigQuery Interaction:**  The function utilizes the `google-cloud-bigquery` Python library:
        * **Client Initialization:** It creates a BigQuery client object to interact with the BigQuery service.
        * **Table Reference:**  Using the `project_name`, `dataset_name`, and `table_name` from the Pub/Sub message, it constructs a table reference, specifying the exact location of the target BigQuery table.
        * **Row Insertion:**  The `client.insert_rows_json()` method is used to insert the constructed row (as a JSON object) into the BigQuery table. The schema of the BigQuery table is expected to match the structure of the row dictionary. 
* **Outputs:** 
    * **BigQuery Data:** A new row of data inserted into the specific BigQuery table (as determined during metadata resolution).
* **External Service Interactions:**
    * **Google Cloud Pub/Sub:** Receives the triggering messages.
    * **Google BigQuery:**  Writes data to BigQuery tables.

**2. Mesonet Weather Data Updater (`mesonet-weather-updater`):**

* **Trigger:** Google Cloud Scheduler, scheduled for hourly execution.
* **Purpose:** To maintain an up-to-date weather record by fetching the latest weather observations from the University of Nebraska-Lincoln Mesonet and appending them to a BigQuery table. 
* **Inputs:**
    * **Mesonet Data URL:** The function is configured with the URL of a CSV file on the Mesonet website that provides recent weather observations (e.g., the past hour's data). 
* **Process:**
    1. **Weather Data Download:** The function uses the `requests` library to download the CSV file from the Mesonet website.
    2. **CSV Parsing and Transformation:** 
        * **CSV Parsing:** The `pandas` library is used to read the downloaded CSV file into a pandas DataFrame. 
        * **Timestamp Conversion:** The timestamp column in the DataFrame, likely in local time, is converted to UTC (Coordinated Universal Time) for consistency. The `pytz` library is commonly used for timezone conversions.
        * **Data Cleaning and Formatting:** Any unnecessary columns are removed from the DataFrame, and data types are adjusted to match the BigQuery table schema (e.g., converting string representations of numbers to actual numerical values).
    3. **BigQuery Insertion:** The function utilizes the `google-cloud-bigquery` library to:
        * **Client Initialization:** Create a BigQuery client object.
        * **Table Reference:** Construct a reference to the target BigQuery table (e.g., `current-weather-mesonet` in the `weather` dataset).
        * **Data Appending:**  The `client.load_table_from_dataframe()` method is used to append the processed pandas DataFrame to the BigQuery table.  The `write_disposition` property is set to `WRITE_APPEND` to add the new data without overwriting existing data.
* **Outputs:** 
    * **BigQuery Data:** Appended rows of weather data in the `current-weather-mesonet` BigQuery table.
* **External Service Interactions:**
    * **Google Cloud Scheduler:** Receives the hourly trigger event.
    * **University of Nebraska-Lincoln Mesonet:** Downloads weather data.

**3. Four-Day Forecast Function (`four-day-forecast-openweathermap`):**

* **Trigger:** Google Cloud Scheduler, scheduled for daily execution. 
* **Purpose:**  To provide a current four-day weather forecast by fetching data from the OpenWeatherMap API and storing it in BigQuery.
* **Inputs:**
    * **OpenWeatherMap API Key:** The function is configured with an API key to access OpenWeatherMap's services.
    * **Location Coordinates:**  The latitude and longitude of the research site are also configured within the function.
* **Process:**
    1. **API Call:** Using the `requests` library, the function makes an API call to OpenWeatherMap, sending the API key and location coordinates as parameters. The API response is typically in JSON format.
    2. **JSON Parsing and Transformation:**
        * **JSON Parsing:**  The function uses the `json` library to parse the API response into a Python dictionary.
        * **Data Extraction:** The function extracts the relevant forecast data from the dictionary.  OpenWeatherMap's API structure provides forecast data at three-hour intervals.
        * **Timestamp Handling:** The timestamps in the forecast data, which are likely in Unix epoch time format, are converted to UTC using the `datetime` library and possibly `pytz` for timezone conversions. 
        * **Data Formatting:** The function structures the extracted data into a list of dictionaries, with each dictionary representing a single three-hour forecast point and containing keys for the timestamp and various forecast variables (temperature, humidity, wind, etc.). 
    3. **BigQuery Integration:**  The function uses the `google-cloud-bigquery` library:
        * **Client Initialization:**  A BigQuery client object is created.
        * **Table Reference:**  The function constructs a reference to the `4-day-forecast-openweathermap` table in the `weather` dataset.
        * **Table Overwrite:**  The `client.load_table_from_json()` method is used to load the formatted forecast data into the table. The `write_disposition` property is set to `WRITE_TRUNCATE`, which overwrites the entire existing table with the new forecast data. This ensures that the table always contains the most current 4-day forecast.
* **Outputs:**
    * **BigQuery Data:** The `4-day-forecast-openweathermap` table is updated with the current four-day weather forecast, replacing any existing forecast data.
* **External Service Interactions:**
    * **Google Cloud Scheduler:**  Receives the daily trigger event.
    * **OpenWeatherMap API:**  Fetches the weather forecast.

**4. CWSI Computation Function (`compute-cwsi`):**

* **Trigger:** Google Cloud Scheduler, with a daily schedule.
* **Purpose:**  To compute the Crop Water Stress Index (CWSI) for each experimental plot, providing insights into the crop's water status and guiding irrigation decisions.
* **Inputs:**
    * **Sensor Data:** Queries BigQuery tables to retrieve canopy temperature (IRT) readings for each relevant plot over the past 24 hours.
    * **Weather Data:** Queries BigQuery for corresponding air temperature (`Ta`), solar radiation (`Rs`), and relative humidity (`RH`) from the `current-weather-mesonet` table for the same 24-hour period. 
* **Process:**
    1. **BigQuery Data Retrieval:**  Using `google-cloud-bigquery`, the function:
        * **Initializes a Client:** Creates a BigQuery client object.
        * **Constructs Queries:**  Forms SQL-like queries to fetch the necessary sensor and weather data from their respective tables. The queries filter data based on plot number and the 24-hour time window.
        * **Executes Queries:**  Runs the queries and retrieves the data as pandas DataFrames. 
    2. **Data Preparation:**
        * **Data Alignment:** The function aligns the sensor data and weather data DataFrames based on their timestamps, ensuring that the calculations are performed on matching data points.
        * **Data Interpolation (if needed):**  If there are missing data points or differences in sampling frequency between the sensor data and weather data, the function might use interpolation techniques (provided by `pandas`) to estimate missing values and create a complete dataset for calculations. 
    3. **CWSI Calculation:** 
        * **Algorithm:** The function implements a specific CWSI algorithm. A common approach is:
           CWSI = (Tc - Ta_wet) / (Ta_dry - Ta_wet)
        
           Where:
           * `Tc` is the measured canopy temperature.
           * `Ta_wet` is the air temperature of a well-watered reference crop under similar conditions (estimated or obtained from historical data).
           * `Ta_dry` is the air temperature of a fully water-stressed crop under similar conditions (also estimated or obtained from historical data).

        * **Implementation:**  The function would likely use `pandas` and `numpy` to apply the CWSI formula across the DataFrame, generating a CWSI value for each row (timestamp).
    4. **BigQuery Update:**
        * **Table Reference:** The function constructs references to the appropriate BigQuery tables for each plot.
        * **CWSI Value Insertion:**  It updates the `cwsi` column in each plot table with the corresponding calculated CWSI values, associating them with the correct timestamps.
* **Outputs:**
    * **BigQuery Data:** The `cwsi` column in the relevant plot tables is updated with the newly calculated CWSI values.
* **External Service Interactions:**
    * **Google Cloud Scheduler:** Receives the daily trigger event.
    * **Google BigQuery:**  Reads sensor and weather data and writes the calculated CWSI values.

**5. SWSI Computation Function (`compute-swsi`):**

* **Trigger:** Google Cloud Scheduler, scheduled for daily execution.
* **Purpose:**  To calculate the Soil Water Stress Index (SWSI) for each experimental plot, assessing the level of water stress in the soil.
* **Inputs:**
    * **Sensor Data:** Queries BigQuery tables for soil moisture readings (TDR) for each relevant plot over the past 24 hours.
* **Process:**
    1. **BigQuery Data Retrieval:**  The function uses `google-cloud-bigquery` to fetch the soil moisture data from the appropriate BigQuery tables, filtering by plot number and the 24-hour time window. The data is retrieved as a pandas DataFrame. 
    2. **SWSI Calculation:**
        * **Algorithm:**  A common SWSI algorithm involves comparing the measured soil moisture (`SM`) to the field capacity (`FC`) and wilting point (`WP`) of the soil:
           SWSI = (FC - SM) / (FC - WP)
           
        * **Implementation:** The function uses `pandas` and `numpy` to apply the SWSI formula to each soil moisture reading in the DataFrame, generating a SWSI value for each timestamp.
    3. **BigQuery Update:**
        * **Table Reference:** The function constructs references to the BigQuery tables for each plot.
        * **SWSI Value Update:** The `swsi` column in each plot table is updated with the corresponding calculated SWSI values. 
* **Outputs:**
    * **BigQuery Data:**  Updated `swsi` values in the relevant plot tables.
* **External Service Interactions:**
    * **Google Cloud Scheduler:** Receives the daily trigger. 
    * **Google BigQuery:**  Reads sensor data and writes SWSI values.

        </content>
    </file>
    <file>
        <name>dashboard.txt</name>
        <path>dashboard.txt</path>
        <content>
You're right to ask for a super-detailed explanation.  Let's break down this dashboard code (`app.py`) step-by-step, focusing on how it retrieves, processes, and visualizes data from BigQuery:

## Irrigation Management Dashboard: A Deep Dive into `app.py`

**1. Setup and Configuration:**

* **Environment Variables:** The code starts by loading environment variables from a `.env` file using `load_dotenv()`.  This file would contain sensitive information, like the path to the Google Cloud service account credentials (`GOOGLE_APPLICATION_CREDENTIALS`). 
* **BigQuery Client Initialization:**  The script uses the `google.cloud.bigquery` library to interact with BigQuery. It:
    * **Loads Credentials:** Reads the service account credentials from the file specified in `GOOGLE_APPLICATION_CREDENTIALS`.
    * **Creates Client:** Initializes a BigQuery client object, providing the credentials and the project ID. 

**2. Helper Functions: The Building Blocks:**

* **`get_all_plots()`:** This function retrieves a list of all available plots from the BigQuery datasets. It does the following:
    1. **Dataset List:** Defines a list of BigQuery dataset names (e.g., `LINEAR_CORN_trt1`, `LINEAR_SOYBEAN_trt2`, etc.) that contain the sensor data.
    2. **Querying Datasets:** Iterates through the dataset list and, for each dataset, queries BigQuery's `__TABLES_SUMMARY__` metadata table to get a list of tables (which represent plots) within that dataset.
    3. **Constructing Plot Names:**  Combines the dataset name and table ID (plot name) into a fully qualified plot name (e.g., `LINEAR_CORN_trt1.plot_5006`).
    4. **Returning Plot List:** Returns a sorted list of all unique plot names.
* **`parse_sensor_name()`:** This function decodes the meaning embedded within the sensor names. It:
    1. **Regular Expression:** Uses a regular expression (`pattern`) to extract the components (sensor type, field number, node, treatment, depth, and timestamp) from a sensor name.
    2. **Matching and Extraction:**  Applies the regular expression to the input sensor name. If there's a match, it returns a dictionary containing the extracted components and their values. If no match, an empty dictionary is returned.
* **`fetch_data()`:** This function fetches data from a specified BigQuery table within a given date range.
    1. **Query Construction:** It dynamically builds a BigQuery SQL query, using f-strings to insert the project ID, dataset, table name, and date range. 
    2. **Query Execution:**  Executes the query using the BigQuery client (`client.query(query)`).
    3. **Data Conversion:** Converts the query results to a `pandas` DataFrame for easier manipulation and visualization. The function also includes the `@st.cache_data` decorator, which caches the results to speed up repeated queries with the same parameters.
* **`get_sensor_columns()`:** This function helps to identify columns in the DataFrame related to a specific sensor type (e.g., all columns that start with "TDR"). It uses a regular expression (`pattern`) to match column names. 
* **`map_irrigation_recommendation()`:**  This function converts irrigation recommendations (which might be stored in different formats in BigQuery) into a consistent numerical format (a float between 0 and 1) for easier visualization.

**3. Streamlit App: The User Interface**

* **Streamlit (`streamlit`)**: Streamlit is a Python library that enables the rapid creation of interactive web applications for data science and machine learning.  This script uses Streamlit to build the dashboard interface. 
* **Layout and Styling:**
    * **Page Configuration:**  `st.set_page_config()` sets the dashboard's title, layout (wide format), and initial sidebar state. 
    * **Dark Theme:** Custom CSS is applied using `st.markdown()` to create a dark theme for the dashboard. 
* **User Input Controls:**
    * **Plot Selection:**  A dropdown (`st.selectbox`) allows the user to select a specific plot from the list retrieved by `get_all_plots()`.
    * **Date Range:**  A date input (`st.date_input`) enables the user to choose a date range for the data visualization.
    * **Refresh Button:**  A button (`st.button`) triggers data refresh (re-execution of the `fetch_data` function) when clicked.
* **Data Fetching and Processing:**
    1. **Dataset and Table Extraction:**  The script extracts the dataset name and table name from the user's selected plot.
    2. **Data Retrieval:**  It calls the `fetch_data` function to retrieve data from BigQuery for the selected plot and date range.
    3. **Timestamp Parsing:** The `TIMESTAMP` column is converted to datetime objects using `pd.to_datetime()`.
* **Dashboard Layout:**  The dashboard's layout is structured using Streamlit's layout features:
    * **Columns:** Two main columns (`left_col`, `right_col`) are created to organize the content.
    * **Grids:** The left column is further divided into a 2x2 grid to hold the charts for weather parameters, temperature, precipitation, and soil moisture. 
* **Visualization with Plotly:** The script uses `plotly.express` and `plotly.graph_objects` to create interactive charts.  Each chart is generated using functions like `go.Figure()`, `px.bar()`, `go.Scatter()`, etc., and customized using `fig.update_layout()` and other Plotly methods.
* **Irrigation Indices:**
    * **Data Extraction and Processing:** The script extracts data for CWSI, SWSI, and ETref (if available) from the DataFrame. The `map_irrigation_recommendation()` function ensures consistent formatting of the irrigation recommendations.
    * **Bar Chart Visualization:** The script uses Plotly to generate a bar chart to display the latest values of the irrigation indices. 
* **Irrigation Recommendation:**  
    * **Gauge Visualization:** The script utilizes Plotly to create an interactive gauge chart to visualize the irrigation recommendation. This provides a more intuitive representation of the irrigation advice. 
* **Footer:** The dashboard includes a footer with copyright information.

**4. External Service Interactions:**

* **Google BigQuery:** The dashboard interacts with BigQuery to retrieve sensor data, weather data, and irrigation metrics. 
* **OpenWeatherMap API:** The forecast function uses the OpenWeatherMap API to fetch forecast data. 

**Key Observations:**

* **Modularity:** The use of helper functions makes the code more organized, readable, and maintainable.
* **Caching:**  The `@st.cache_data` decorator on the `fetch_data` function improves performance by caching query results.
* **Interactivity:**  Streamlit's input widgets and Plotly's interactive charts enhance the dashboard's user experience.
* **Integration:**  The dashboard seamlessly integrates with Google Cloud services (BigQuery, Pub/Sub, Cloud Scheduler) and external APIs (OpenWeatherMap). 

This detailed analysis provides a clear picture of how the dashboard works, demonstrating its reliance on well-structured data from BigQuery, the use of Python libraries for processing and visualization, and the creation of an interactive and informative interface with Streamlit. 

        </content>
    </file>
    <file>
        <name>equipment_used.yaml</name>
        <path>equipment_used.yaml</path>
        <content>
hardware_components:
  - name: "Adjustable LM2596S DC-DC Buck Converter"
    manufacturer: "Frienda"
    location: "Ann Arbor, Michigan, USA"
    technical_specs:
      input_voltage_range: "4.0V - 40V"
      output_voltage_range: "1.25V - 37V"
      output_current: "2A (stable), 3A max"
      conversion_efficiency: "92%"
    use: "Voltage stabilization and regulation"
    price: "$1.00"

  - name: "TDR-315N Time Domain Reflectometer"
    manufacturer: "Acclima"
    location: "Meridian, Idaho, USA"
    measurement_parameters:
      measured_data: 
        - "Volumetric water content (VWC)"
        - "Relative permittivity"
        - "Bulk electrical conductivity (BEC)"
        - "Temperature"
      output: "SDI-12"
      range: 
        - VWC: "0% to 100%"
        - Temperature: "-40°C to 55°C"
        - BEC: "0 to 3000 µS/cm"
      resolution: 
        - VWC: "0.1%"
        - Temperature: "0.1°C"
        - BEC: "1 µS/cm"
      accuracy: "±2% for VWC"
      supply_voltage_range: "4.2V to 15V"
      power: "118 mA at 12V, idle current < 10 µA"
    use: "Accurate soil moisture measurement in various soil types"
    price: "$270.50"

  - name: "SI-4H1-SS Infrared Radiometer"
    manufacturer: "Apogee Instruments"
    location: "Logan, Utah, USA"
    measurement_parameters:
      measured_data: "Surface temperature"
      output: "SDI-12"
      range: "-40°C to 85°C"
      resolution: "0.1°C"
      accuracy: "±0.2°C"
      supply_voltage_range: "5V to 24V"
      power: "< 0.5 W"
    use: "Temperature measurement for environmental monitoring"
    price: "$890.00"

  - name: "IoT Node pHAT with LoRa"
    manufacturer: "Pi Supply"
    location: "Canterbury, England, UK"
    technical_specs:
      frequency_support: ["EU 863 - 870 MHz", "US 902 - 928 MHz"]
      communication: "UART using 3 GPIO Pins"
    use: "IoT node creation compatible with The Things Network"
    price: "$36.51"

  - name: "Raspberry Pi 4 Model B"
    manufacturer: "Raspberry Pi Foundation"
    location: "Cambridge, England, UK"
    technical_specs:
      processor: "Broadcom BCM2711, Quad core Cortex-A72 (ARM v8) 64-bit SoC @ 1.8GHz"
      memory_options: ["1GB", "2GB", "4GB", "8GB LPDDR4-3200 SDRAM"]
    use: "General-purpose computing, IoT projects, educational purposes"
    price: "$61.55"

  - name: "NERMAK 12V 16Ah Lithium Iron Phosphate Battery"
    manufacturer: "NERMAK"
    location: "Shenzhen, Guangdong, China"
    technical_specs:
      voltage: "12.8V"
      capacity: "16Ah (204.8Wh)"
      cycle_life: "2000+ cycles"
    use: "High energy density backup power solution"
    price: "$50.00"

  - name: "IOT-G67 Outdoor LoRaWAN Gateway"
    manufacturer: "Linovision"
    location: "Shenzhen, Guangdong, China"
    technical_specs:
      processor: "Quad-core industrial processor"
      channels: "8 half/full-duplex channels"
      LoRa_chip: "SX1302"
    use: "Long-range communication for IoT networks in outdoor applications"
    price: "$769.00"

  - name: "30 Watt 12V Solar Panel Kit"
    manufacturer: "SUNSUL"
    location: "Ningbo, Zhejiang, China"
    technical_specs:
      power: "30 Watt"
      efficiency: "Up to 23%"
      charge_controller: "12V/24V 5A PWM Solar Charge Controller"
    use: "12V battery charging and various DC applications"
    price: "$59.99"

  - name: "CR800 Measurement and Control Datalogger"
    manufacturer: "Campbell Scientific"
    location: "Logan, Utah, USA"
    measurement_parameters:
      measured_data: "Environmental data logging"
      output: "RS-232 ($7) or USB"
      range: "6 differential or 12 single-ended analog inputs"
      accuracy: "High accuracy for environmental monitoring"
      supply_voltage_range: "7 to 16V"
      power: "1.2 mA (active)"
    use: "Environmental data logging and control"
    price: "$1,152.00 per unit"

        </content>
    </file>
    <file>
        <name>google_cloud_services_explained.txt</name>
        <path>google_cloud_services_explained.txt</path>
        <content>

## GCP Services in Detail: A Deep Dive

**Google Cloud Virtual Machine (VM):** 

**General Purpose:** A Google Cloud VM provides a flexible, virtualized computing environment. This allows researchers to run applications and manage workloads on customizable instances within Google's robust and secure infrastructure. 

**Specific Role in Project:**  A GCP VM instance, running the Ubuntu 20.04 LTS operating system, served as a dedicated server for a vital task: bridging the LoRaWAN network and the Google Cloud ecosystem. This bridge was implemented as a custom-developed MQTT forwarder application named `emqx_to_pubsub.py`. 

* **Bridging the Gap:** This Python script used the `paho-mqtt` library to securely connect to the EMQX Cloud MQTT broker. It subscribed to the `device/data/uplink` topic, which received data from the LoRaWAN gateway.
* **Message Processing and Transformation:** Incoming messages from the EMQX broker were carefully processed:
    1. **UTF-8 Decoding:** The raw message payload was first decoded from its UTF-8 representation into a Python dictionary.
    2. **Base64 Decoding:** The `data` field within the decoded JSON held the sensor data encoded in base64. The script extracted and decoded this string, revealing the original JSON payload as sent from the Raspberry Pi node in the field. 
    3. **Hash Resolution and Enrichment:** This original payload from the Raspberry Pi contained key-value pairs where the keys were concise, three-digit sensor hashes. These hashes, though efficient for transmission, lacked the complete sensor metadata needed for accurate storage and analysis. The script employed a YAML configuration file, `sensor_mapping.yaml`, identical to the one on the Raspberry Pi.  This file enabled the forwarder to map each hash to its corresponding sensor's full ID (`sensor_id`), field location, treatment group, plot number, and the targeted BigQuery dataset and table. 
    4. **Constructing Pub/Sub Messages:**  For each sensor hash and reading in the original payload, the forwarder meticulously constructed a new JSON message formatted for seamless integration with Google Cloud Pub/Sub.  These messages included: 
        * `timestamp`: The exact time of the sensor reading (as provided by the Raspberry Pi).
        * `sensor_id`:  The full sensor ID, no longer just a hash.
        * `value`:  The numerical sensor reading.
        * `project_name`: "crop2cloud24", the Google Cloud project designated for this research.
        * `dataset_name`: A structured dataset name derived from the sensor's field and treatment: "{field}_trt{treatment}".
        * `table_name`:  The exact BigQuery table to receive the data, determined by the plot: "plot_{plot_number}".
* **Pub/Sub Publishing:**  The script then used Google's `google-cloud-pubsub` Python library to publish these enriched JSON messages to the `tester` topic on Google Cloud Pub/Sub.
* **Reliability and Robustness:** The `emqx_to_pubsub.py` script incorporated error handling and logging via Python's `logging` module. This captured exceptions during MQTT interaction or Pub/Sub communication, writing detailed logs to `app.log` for diagnostics and system monitoring. An exponential backoff retry mechanism was also implemented to handle transient network errors, ensuring data delivery even in the face of temporary disruptions.

**Google Cloud Pub/Sub:**

**General Purpose:** Pub/Sub is Google's real-time, highly scalable messaging service. It facilitates asynchronous communication between various components of a system, serving as a central hub for data distribution. 

**Specific Role in Project:** Google Cloud Pub/Sub took on the essential role of a message queue. This decoupled the data flow, allowing the MQTT forwarder to deliver messages without being directly tied to the downstream processing by Cloud Functions. The `tester` topic on Pub/Sub became a critical trigger point. Each time the forwarder published a message to this topic, Pub/Sub automatically invoked the corresponding Cloud Function designated to handle sensor data ingestion. This event-driven architecture provided efficiency and flexibility.

**Google BigQuery:**

**General Purpose:** BigQuery is Google Cloud's fully managed, petabyte-scale data warehouse solution. It excels in storing and analyzing massive datasets with its serverless architecture and high-performance query engine.

**Specific Role in Project:** BigQuery acted as the definitive data repository for this research project. A meticulously structured organization was implemented within BigQuery:
* **Datasets:** The project utilized separate datasets to logically categorize data by both field type (LINEAR_CORN or LINEAR_SOY) and irrigation treatment.
* **Tables:** Within each dataset, dedicated tables were created for each individual experimental plot.  These tables had a well-defined schema:
    * `TIMESTAMP`: The precise time of the reading.
    * Raw Sensor Readings: Fields for each sensor type (e.g., `IRT5023A1xx24` representing an infrared temperature reading in plot 5023).
    * `cwsi`: The computed Crop Water Stress Index for the plot.
    * `swsi`: The computed Soil Water Stress Index for the plot.
    * Weather Data: Fields for meteorological variables (e.g., `Ta_2m_Avg` representing average air temperature).

This organized structure facilitated efficient storage, retrieval, and analysis of all collected data, both raw and processed.

**Google Cloud Functions:**

**General Purpose:** Cloud Functions are Google's serverless compute offering, enabling developers to run event-driven code without managing servers. Functions are triggered by a variety of events, including HTTP requests, Pub/Sub messages, or scheduled invocations.

**Specific Role in Project:** Cloud Functions formed the core of data handling, processing, and analysis. Key functions included:

* **Data Ingestion:** A vital function, triggered by new messages arriving on the `tester` Pub/Sub topic, was responsible for ingesting the enriched sensor data delivered by the MQTT forwarder.
    * **Message Parsing:** The function parsed the incoming JSON message to extract the sensor reading, timestamp, and the metadata derived from the hash mapping.
    * **BigQuery Insertion:** Using the powerful `google-cloud-bigquery` Python library, the function inserted the data into the correct BigQuery table. The dataset and table were determined using the `dataset_name` and `table_name` fields that were added by the forwarder based on the sensor hash mapping.
* **Weather Data Acquisition:** The scheduled function named `mesonet-weather-updater` provided a steady stream of current weather data:
    * **Data Retrieval:** This function used Python's `requests` library to access the University of Nebraska-Lincoln Mesonet website, downloading the latest weather data as a CSV file.
    * **Data Transformation:**  The raw CSV was parsed, processed, and converted into a format aligned with the schema of the `current-weather-mesonet` BigQuery table.
    * **BigQuery Update:**  The function appended the newly retrieved and transformed weather data to this designated table, ensuring a constantly updated weather record.
* **Forecast Data Acquisition:**  The `four-day-forecast-openweathermap` function, also scheduled, provided periodic weather forecasts:
    * **API Call:** This function leveraged the OpenWeatherMap API (via the `requests` library) to fetch a 4-day forecast for the research site.
    * **Processing and Formatting:**  The received forecast data was processed and formatted to conform to the schema of the `4-day-forecast-openweathermap` table in BigQuery. 
    * **BigQuery Update:**  The function updated the forecast data in the specified BigQuery table, overwriting existing forecast data to keep the information current. 
* **Irrigation Metric Computation:** The heart of the irrigation management aspect was handled by two scheduled Cloud Functions: `compute-cwsi` and `compute-swsi`.
    * **Data Retrieval:**  These functions retrieved the necessary sensor data (e.g., IRT, TDR) and weather data from their respective BigQuery tables.
    * **CWSI and SWSI Calculations:**  Using `pandas` and `numpy` (robust Python libraries for data manipulation and numerical analysis), the functions calculated the Crop Water Stress Index (CWSI) and Soil Water Stress Index (SWSI) for each plot.
    * **BigQuery Update:**  The calculated CWSI and SWSI values were written back to the corresponding plot tables in BigQuery, providing up-to-date irrigation guidance.

**Google Cloud Scheduler:**

**General Purpose:** Cloud Scheduler is a fully managed cron job scheduler within Google Cloud. It enables the automation of recurring tasks, defining specific schedules and execution times.

**Specific Role in Project:** Cloud Scheduler was the driving force behind the automation of essential periodic tasks:

* **Hourly Weather Updates:** The `mesonet-weather-updater` function was rigorously scheduled to execute every hour. This guaranteed that the latest weather observations were regularly retrieved and made available for analysis within BigQuery.
* **Daily Forecast Updates:** The `four-day-forecast-openweathermap` function was scheduled for daily execution, updating the forecast data in BigQuery and ensuring that predictions remained relevant.
* **Daily Irrigation Metric Calculation:**  The `compute-cwsi` and `compute-swsi` functions were also set for daily execution. This daily update ensured that the CWSI and SWSI values were consistently computed using the most recent sensor readings and weather data, providing the foundation for informed, data-driven irrigation decisions.

By combining these carefully designed and interconnected GCP services, the research project achieved a robust, automated, and scalable system for agricultural monitoring and irrigation management. 

        </content>
    </file>
    <file>
        <name>sensor_nomenclature_and_hashing.txt</name>
        <path>sensor_nomenclature_and_hashing.txt</path>
        <content>
You're right to focus on the sensor nomenclature and hashing – it's a critical and clever optimization. Here's a detailed explanation and a table to illustrate:

##  Sensor Nomenclature and Hashing:  Decoding the Efficiency

**The Challenge:**  In an IoT system designed for remote agricultural monitoring, data transmission efficiency is paramount.  Sending lengthy sensor identifiers over a low-power, long-range network like LoRaWAN can significantly impact battery life and overall system performance. 

**The Solution:** A two-part strategy combines a highly descriptive sensor naming convention with a data-reducing hashing mechanism.

**1.  Comprehensive Sensor Nomenclature:**

Each sensor is given a unique identifier that encodes its essential attributes. This nomenclature acts as a sensor's "fingerprint," immediately revealing its key characteristics:

* **Sensor Type:** A three-letter code indicating the type of sensor:
    * `IRT`: Infrared Thermometer (for canopy temperature)
    * `TDR`: Time Domain Reflectometry (for soil moisture)
    * `WAM`: Watermark (for soil moisture, though not used in the current examples)
    * `SAP`: Sapflow (for measuring sap flow in plants, not used here)
    * `DEN`: Dendrometer (for measuring stem diameter, not used in the initial examples) 

* **Field Number:** A four-digit number uniquely identifying the field where the sensor is deployed.

* **Node:** A single letter (A, B, C, D, or E) representing the Raspberry Pi node to which the sensor is connected. 

* **Treatment:** A single digit representing the irrigation treatment being applied in the plot where the sensor is located. 

* **Depth:** A two-digit number (e.g., 06, 18, 32) indicating the sensor's depth in inches. For sensors where depth is not applicable (like IRTs), "xx" is used as a placeholder.

* **Timestamp:**  A two-digit year code (e.g., '24') denoting the year the sensor was installed. This also distinguishes sensors in the "linear" field layout (LR).

**Example:**  The sensor ID `TDR5001C30624` represents:
* A TDR sensor for measuring soil moisture.
* Deployed in field 5001.
* Connected to Raspberry Pi node C.
* Located in a plot under irrigation treatment 3.
* Installed at a depth of 6 inches.
* Installed in the year 2024.

**2.  Hashing for Data Reduction:**

The detailed sensor names, while informative, are too lengthy for efficient LoRaWAN transmission. This is where hashing comes in. Each unique sensor name is assigned a short, three-digit hash. 

* **The `sensor_mapping.yaml` File:** This YAML file holds the critical mapping between the full sensor names and their corresponding hashes. 
* **Hashing Algorithm:** The specific algorithm used to generate the hashes is not specified in the provided context, but it must be a consistent, one-way function. This means that the same sensor name will always produce the same hash, but it's computationally infeasible to derive the sensor name from the hash. Common hashing algorithms include MD5, SHA-1, or SHA-256.
* **Data Transmission:**  Instead of sending the full sensor name, only the three-digit hash is transmitted. 
* **Metadata Reconstruction:**  At the receiving end (the MQTT forwarder on the VM and the data ingestion Cloud Function), the `sensor_mapping.yaml` file is used to reverse the mapping, reconstructing the complete sensor metadata from the hash.

**Illustrative Table:**

| Sensor Hash | Sensor ID (Nomenclature) | Sensor Type | Field | Node | Treatment | Depth (inches) | Installation Year |
|---|---|---|---|---|---|---|---|
| "001" | IRT5001C3xx24 | Infrared Thermometer | 5001 | C | 3 | N/A | 2024 |
| "002" | IRT5003C2xx24 | Infrared Thermometer | 5003 | C | 2 | N/A | 2024 |
| "003" | IRT5010C1xx24 | Infrared Thermometer | 5010 | C | 1 | N/A | 2024 |
| "004" | TDR5003C20624 | Time Domain Reflectometry | 5003 | C | 2 | 06 | 2024 | 
| "005" | TDR5003C21824 | Time Domain Reflectometry | 5003 | C | 2 | 18 | 2024 |
| ... | ... | ... | ... | ... | ... | ... | ... |

**The Benefits:**

* **Data Efficiency:**  Significantly reduces the amount of data that needs to be transmitted over LoRaWAN, conserving battery power and bandwidth.
* **Data Integrity:** The descriptive nomenclature ensures that, even after hashing, the full metadata for each sensor can be recovered.
* **Scalability:** The system can easily accommodate new sensors by assigning them unique names and hashes, updating the `sensor_mapping.yaml` file accordingly.

**Key Takeaway:** The sensor nomenclature and hashing mechanism provide a clever balance between data richness, transmission efficiency, and system scalability, making it well-suited for an IoT-based agricultural monitoring system. 

        </content>
    </file>
    <file>
        <name>software_interactions_explained.txt</name>
        <path>software_interactions_explained.txt</path>
        <content>
You're absolutely right - the interconnection between the Campbell datalogger, the Raspberry Pi, and the sensor mapping is where the true magic (and often the headaches!) of this system lie.  Let's dive into those critical details with even more rigor!

## Software Operations: A Symphony of Interconnection

**1. Campbell CR800 Datalogger Software:**

* **CRBasic Program:** The CR800 runs a program meticulously crafted in CRBasic, Campbell Scientific's specialized programming language for data acquisition. This program governs the datalogger's every action. 
* **Sensor Specificity:** Crucial to the system's accuracy is the program's ability to target specific sensors.  This is achieved through:
    * **Variable Declarations:**  Each sensor connected to the CR800 has a corresponding variable declared in the program. This variable acts as a container to store the sensor's readings. The variable name often encodes information about the sensor (e.g., `TDR5023A10624` could represent a Time Domain Reflectometry (TDR) sensor in plot 5023, treatment 1, at a depth of 6 inches, installed in 2024).
    * **SDI-12 Addressing:** The CRBasic program assigns a unique SDI-12 address to each sensor. Think of this address as a sensor's phone number.  When the datalogger wants a reading from a particular sensor, it uses this address to call that sensor. This mapping between sensor variables and SDI-12 addresses is defined as constants within the CRBasic program, ensuring accurate communication. 
* **Measurement Cycle:** The program orchestrates a precise measurement cycle, typically set to repeat every 5 minutes. Here's what happens in each cycle:
    * **Sensor Queries:** The CR800, using the SDI-12 protocol, sends commands to each sensor, querying them for their current readings.  It selects the right sensor based on its predefined SDI-12 address. 
    * **Data Reception and Storage:** Each sensor responds to the CR800's query with its current measurement. The datalogger receives this reading and stores it in the corresponding variable declared in the program.
    * **Averaging:**  To reduce noise and provide a more representative measurement, the program averages the raw sensor readings over a longer interval, usually 30 minutes. 
    * **Data Logging:**  Finally, the averaged sensor readings, along with their associated timestamps, are stored in the CR800's internal memory, creating a historical record of the environmental data.

**2. Raspberry Pi Software: A Deeper Look into the Core**

* **Python Script:  The Pi's Inner Workings:**  The Raspberry Pi's custom Python script serves as the data maestro, responsible for collecting, processing, and transmitting the precious sensor readings. 
* **Crucial Libraries:**
    * **pycampbellcr1000:** This library is the key to unlocking the CR800's data.  It acts as a translator, allowing the Raspberry Pi to understand and communicate with the datalogger using Python.
    * **rak811:**  This library provides the tools to control the RAK811 LoRa module, turning the Raspberry Pi into a LoRaWAN node capable of long-range wireless communication. 
    * **yaml:**  The `yaml` library is essential for parsing the `sensor_mapping.yaml` file, which holds the vital map between sensor hashes and their descriptive metadata.
    * **json:**  The `json` library enables the script to encode the sensor data into JSON format, the lightweight and versatile data exchange format perfect for LoRaWAN transmission.
* **Script Logic:  Unmasking the Interaction:**
    1. **System Time Synchronization:**  The script starts by making sure the Raspberry Pi's clock is accurate. It synchronizes its system time with a network time server using the `timedatectl` command, ensuring all data has reliable timestamps.
    2. **Datalogger Handshake:** The script uses `pycampbellcr1000` to initiate a connection with the CR800 datalogger via USB. It reads the datalogger's configuration (port, baud rate) from `config.yaml`. The `CR1000.from_url()` function from `pycampbellcr1000` is likely used to establish this connection. 
    3. **Table Discovery:**  The script then requests a list of data tables from the CR800 using the `datalogger.list_tables()` function. It intelligently filters this list, identifying the table that holds the averaged sensor readings, ignoring any system tables.
    4. **Avoiding Data Duplication:** To maintain a clean and continuous data record, the script queries a local SQLite database on the Raspberry Pi.  It retrieves the timestamp of the last data point it fetched, using this information to determine the start time for collecting new data from the CR800. It also uses a time window (e.g., the past 2 days) to ensure it captures any recent data that might have been missed.
    5. **Data Download: The Nitty Gritty:**
        * **`datalogger.get_data()`:** This function, from `pycampbellcr1000`, is the workhorse. The script uses it to fetch data from the selected table within the specified time range. This function likely handles the low-level communication, sending commands to the CR800 to read data from its memory and receive the data over USB.
        * **CRBasic Data Format:** The CR800 returns the data in a format specific to CRBasic, possibly a tabular structure.
        * **Python-Friendly Transformation:** The script then transforms the retrieved data into a Python list of dictionaries. Each dictionary represents a single data point, with keys for the timestamp ("Datetime" in CRBasic, renamed to "TIMESTAMP") and each sensor's readings.
    6. **Data Cleansing:** The script tidies up the data:
        * **Timestamp Consistency:** The "Datetime" field from the CR800 is renamed to "TIMESTAMP" for standardization.
        * **Missing Value Handling:**  The script searches for any `NaN` (not a number) values in the sensor readings. These are replaced with a placeholder value, like -9999, to clearly indicate missing data.
    7. **From Names to Hashes: The Magic of Mapping:**
        * **The `sensor_mapping.yaml` File:** This YAML file holds the map between sensor names (as they appear in the CR800 data) and their three-digit hashes.  It's crucial for both data reduction (for transmission) and data reconstruction at the receiving end.
        * **Hashing:** The script loads the mapping from `sensor_mapping.yaml` and uses it to convert the full sensor names to their corresponding hashes. This drastically reduces the size of the data to be transmitted. 
    8. **Chunking for LoRaWAN Efficiency:**
        * **Packet Size Constraints:** LoRaWAN packets have size limitations. To ensure data can be transmitted without exceeding these limits, the script breaks the hashed sensor data and timestamp into smaller chunks. 
        * **Chunk Formation:** The specific algorithm for chunking might involve dividing the data into groups of a certain number of sensor readings or based on the total estimated size of the encoded JSON payload. 
    9. **LoRaWAN Transmission: Sending Data to the Cloud:**
        * **LoRa Module Initialization:** The script utilizes the `rak811` library to configure and initialize the RAK811 LoRa module. It sets parameters read from `config.yaml`, including:
            * **Region:** This defines the LoRaWAN frequency band used (e.g., US915).
            * **Data Rate:**  The speed at which data is transmitted (a balance between range and power consumption).
            * **Device Address:**  The unique identifier for the LoRa node.
            * **Application Session Key & Network Session Key:** Encryption keys for secure communication.
        * **Joining the Network:** Once configured, the script instructs the LoRa module to join the LoRaWAN network. 
        * **Transmitting the Chunks:** Each data chunk is encoded as a JSON payload using the `json` library. The script then uses the `rak811` library to send these payloads through the LoRa module. 
        * **Transmission Scheduling:**  To distribute the transmission load and prevent congestion, the script introduces a random delay between sending each chunk.  This delay falls within the configured transmission window and adheres to the minimum interval between transmissions.
    10. **Local Data Storage:**  For redundancy and local data access, the script stores the retrieved and processed sensor data in a local SQLite database on the Raspberry Pi.

**4. The 30-Minute Wake-Up Call: Systemd Timer Magic**

* **The `systemd` System:** The Raspberry Pi uses the `systemd` system to manage services and timers. This provides a robust and reliable way to schedule tasks and handle automatic execution.
* **Systemd Timer:** During the node setup (using `setup.py`), a systemd timer is created specifically to trigger the data collection script every 30 minutes. 
* **Timer Configuration:** The timer's configuration defines:
    * **Interval:**  30 minutes.
    * **Target Service:**  The systemd service that runs the Python data collection script.
* **How It Works:**  The systemd timer runs in the background and, at the specified interval, activates the data collection service, which in turn executes the Python script.  Once the script completes its tasks (fetching, processing, and transmitting data), the systemd service deactivates, and the Raspberry Pi effectively goes back to sleep until the timer triggers it again.

**3. Raspberry Pi Node Setup (setup.py)**

* **Reproducible Deployment:** Because the project involved multiple Raspberry Pi nodes, a setup script (`setup.py`) was created to make the deployment process reproducible and efficient across all nodes. 
* **Key Tasks of setup.py:**
    1. **Dependency Installation:** The script installs all required system packages (like `python3-venv`, `python3-pip`, `git`, etc.) and Python libraries (using `pip install -r requirements.txt`) on the Raspberry Pi.  This ensures all nodes have the same software environment.
    2. **Virtual Environment Setup:** The script sets up a Python virtual environment, providing an isolated space for the project's dependencies, preventing conflicts with other software on the Raspberry Pi.
    3. **Systemd Service Creation:** The script creates a systemd service to automatically run the data collection script on startup and to manage its execution. This includes defining:
        * **Service Description:** A clear description of the service.
        * **Working Directory:** The path to the project's directory on the Raspberry Pi.
        * **Execution Command:** The command to execute the Python data collection script.
        * **User and Group:** The user and group under which the service will run (usually the user who set up the Raspberry Pi).
        * **Restart Behavior:**  How the service should restart if it encounters errors (e.g., restart on failure).
        * **Logging:** Where the service logs should be written.
    4. **Systemd Timer Creation:** The script creates a systemd timer that triggers the data collection service at specified intervals (every 30 minutes in this case). 
    5. **Reboot Counter:**  A reboot counter is set up to track how many times the Raspberry Pi has rebooted. This is helpful for diagnosing potential issues if a node experiences frequent reboots.
    6. **Systemd Reboot Service:**  A special systemd service is created that monitors a specific file.  If this file exists, it triggers a system reboot. This mechanism allows the data collection script to request a reboot if necessary (e.g., after a certain number of errors or in case of a hardware issue).
    7. **LoRa HAT Configuration:**  The script configures the LoRa HAT, including:
        * **Enabling UART:**  The UART (Universal Asynchronous Receiver/Transmitter) is enabled on the Raspberry Pi to allow communication with the LoRa HAT.
        * **Disabling Bluetooth:** Bluetooth is disabled to prevent interference with the LoRa communication.
        * **Udev Rules:** A udev rule is set up to give the appropriate user group permissions to access the LoRa HAT's serial port.
    8. **Permission Management:**  The script sets the correct file permissions and ensures the user who set up the Raspberry Pi has the necessary privileges to interact with the project files, services, and the LoRa HAT.

**4. EMQX MQTT Broker:**

* **Version:** EMQX 5.0 (or the version specified in the project).
* **Deployment:** EMQX Cloud, a fully managed cloud-based MQTT service.
* **Purpose in System:**
    * **Data Reception:** The gateway transmits the received LoRa packets to the EMQX Cloud broker, publishing the data to the topic `device/data/uplink`.
    * **Message Routing:** EMQX's main function is to efficiently route these messages.  The custom MQTT forwarder running on the GCP VM is subscribed to the `device/data/uplink` topic. This ensures that all sensor data arriving at the broker is immediately forwarded to the VM.

**Rationale for EMQX:**
* **Scalability:** EMQX Cloud is designed to scale with the demands of an IoT deployment, handling a vast number of connected devices and messages.
* **Reliability:**  As a fully managed cloud service, EMQX offers high availability and fault tolerance.
* **Security:** EMQX supports TLS/SSL encryption, ensuring secure communication between the LoRa gateway, the broker, and any connected clients.

**The Crucial Interconnection:**
* **Node-Specific Sensor Configuration:** The link between the CR800 program and the Raspberry Pi's `sensor_mapping.yaml` file is established during the initial setup of each node. The CR800 program defines which sensors are connected to a particular datalogger and their SDI-12 addresses. The `sensor_mapping.yaml` file, which must be identical on the Raspberry Pi and the VM, translates the sensor hashes to this full metadata. 
* **Table Recognition:**  The Raspberry Pi script does not need to know the specific sensor names or addresses in advance.  It relies on the CR800 to provide the data table, and then uses the timestamps to retrieve the correct data.  The sensor names are only used later for hashing and mapping to metadata.

By understanding these intricate connections and the software operations that drive them, you gain a comprehensive view of how the system functions as a cohesive unit, gathering, processing, transmitting, and storing the data essential for informed irrigation management. 

        </content>
    </file>
    <file>
        <name>study_region&experimental_design.yaml</name>
        <path>study_region&experimental_design.yaml</path>
        <content>
location:
  name: University of Nebraska-Lincoln, West Central Research, Extension, and Education Center (WCREEC)
  city: North Platte
  state: Nebraska
  country: USA
  coordinates: 41.089282, -100.773808



climate:
  type: semi-arid
  precipitation_pattern: 80% of annual precipitation occurs between late-April and mid-October
  reference: Payero et al., 2009

soil:
  type: Cozad silt loam
  field_capacity: 0.29 m3 m−3
  permanent_wilting_point: 0.11 m3 m−3
  reference: Klocke et al., 1999

irrigation_system:
  type: linear move Variable Rate Irrigation (VRI)
  treatments:
    - name: Crop Water Stress Index (CWSI) using canopy temperatures
      details: Only one treatment was used for monitoring CWSI based on canopy temperatures.
    - name: Combination of Volumetric Water Content (VWC) and Canopy Temperatures
      details: This treatment used three depths of volumetric water content and canopy temperatures, scheduled using threshold values for both Soil Water Stress Index (SWSI) and CWSI.
    - name: Soil Water Stress Index (SWSI) thresholds only
      details: This treatment relied solely on threshold values for SWSI to guide irrigation decisions.
    - name: Fuzzy Logic System using multiple indices
      details: The fourth treatment employed evapotranspiration, crop water stress, soil water stress indices, and plant growth stages in a fuzzy logic system for irrigation decision-making.
  experimental_design:
    type: randomized complete block design (RCBD)
    blocks: 3
    plot_dimensions: 9.1 by 72.2 m
    treatments_count: 4
    special_note: One treatment was exclusively used for crop water stress index monitoring.

scheduling_criteria:
  data_sources:
    - Crop Water Stress Index (CWSI)
    - Soil Water Stress Index (SWSI)
    - Evapotranspiration (ET)
  fuzzy_logic_system:
    description: Sensor data collected from canopy temperatures, soil moisture, and crop stages were transformed into CWSI and ET values for irrigation scheduling, incorporating plant growth stages into a fuzzy logic-based decision system.
  irrigation_frequency: ""
  sensor_system_integration: true



references:
  - citation: Payero et al., 2009
    details: "Effect of timing of a deficit-irrigation allocation on corn evapotranspiration, yield, water use efficiency and dry mass. Agricultural Water Management, 96(10), 1387-1397. https://doi.org/10.1016/j.agwat.2009.03.022"
  - citation: Klocke et al., 1999
    details: "Nitrate leaching in irrigated corn and soybean in a semi-arid climate. Transactions of the ASAE, 42(6), 1621-1630. https://doi.org/10.13031/2013.13328"



        </content>
    </file>
    <file>
        <name>system_description.txt</name>
        <path>system_description.txt</path>
        <content>
<overall_system_description>
here's the plan so I have a research project that I'm in the process of converting to a research paper. Here is the overall context. It is it is a system for automated data collection and processing for agricultural monitoring for irrigation scheduling. So I'm going to begin from the source of the data. There are I'm gonna begin from the source of the data. There are TDR sensors, that is T D R, placed at three different depths within the soil, six inches. depths within the soil. 6 inches, 18 inches and 32 inches each. 6 inches apart there are canopy temperature sensors raised one meter above the canopy and placed at a 45 degree angle pointing towards the crop and then there are these four sensors. Dendrometers placed around the plant stem. So there are these four sensors. The TDR, that is TDR and IRT, infrared temperature sensors. TDR stands for time reflectometry sensors. Those use the SDI-12 communication protocol. Meanwhile, the dendrometer is an analog sensor. So, each of these are connected to a data logger, CR800 data logger from Campbell Scientific. The data logger will sample the data every five minutes and then average that data over the course of 30 minutes and store the data every 30 minutes. Then a Raspberry Pi is plugged into the data logger via an RS-232 to USB adapter which allows the Raspberry Pi to talk with the Campbell scientific data logger and interface with it using the PI Campbell 1000 PI Campbell CR 1000 sorry library which allows the communication So the Raspberry Pi itself and then it will also send that data using a LoRa module attached to its pins right so it's called a LoRa hat from Pi supply and it basically turns the Raspberry Pi Raspberry Pi 4 into a LoRa node one challenge is that the specific nomenclature that is used to identify each sensor in the field and you will see this nomenclature in the sensor data YAML. This specific nomenclature helps one identify the sensor and immediately tells you all sensor metadata but then it's too much to transfer over LoRa because the bigger the packet size the more power it takes to send and the fewer sensor data you can send together so one needs to be able to at the same time easily identify sensors and to be able to send sensors sensor data over the LoRa network with a small header. So a sensor name to hash mapping is created so that the sensors can be sent only with a three digitdigit hash that is sufficient to encode all sensor data sent over the network. So the data is gotten from the logger. Each header is assigned a hash, a three-digit hash that maps it to all the required metadata for that sensor. And the metadata includes the sensor name as well as all the Google BigQuery credentials that is needed for data piece of sensor data is encoded within the the mapping file which is a yaml file and with that three digit hash you can basically know everything you need to know about the sensor. So the sensor data is sent from the Raspberry Pi through the LoRa hat as simple JSON packets, right? So the JSON is encoded into base 64, and we're only using, we're sending the sensor data one packet or the data from one sensor at a time. So we have just a timestamp to identify when the data from one sensor at a time. So we have just a timestamp to identify when the data was taken. And then we have the... the actual sensor data so those two are put into a json converted to base 64 sent over LoRa then we have a gateway which is about a mile from the sensor a LoRa then we have a gateway which is about a mile from the sensor also like all the kind of LoRa settings are in the code so there's a gateway mounted where all of these sensors send their data to it. The water is so clear that I can't see the sea. I'm going to take a walk in the water. I'm going to take a walk in the water. I'm going to take a walk in the water. I'm going to take a walk in the water. send their data to the gateway and then the gateway maybe we can also go through the gateway setup but the gateways got to like fiber optic poles you know gateway receives the signals and then kind of it's also subscribed to an mqtt um broker the one we're using is EMQX so it basically takes each packet and forwards it to the EMQX MQTT broker all right and then the EMQX MQTT broker receives those packages and forwards them on. So actually here, I'm not sure about the termination. Like what exactly is EMQX in this context? But anyway, I have like another MQTT thing running on the virtual machine using Mosquito and it will receive those packets from EMQX. It's running on the virtual machine and the virtual machine is perpetually running. Receive those packets, and then it will forward the packets to Google PubSub thing, right? So the PubSub topic and really dig into the details of this. and really dig into the details of this. The PubSub topic is triggered by kind of a cloud function. So the PubSub triggers a cloud function. So when it receives like the data packets, um it will It will then write those data packets to the required BigQuery table. But then, remember, we had a hash, right? So where does that hash get kind of used and kind of converted back to the relevant data? That happens at the level of the virtual machine. So the virtual machine will see a hash and a corresponding timestamp. And then it will use the uploaded metadata file, YAML metadata file which is exists at the level of the Raspberry Pi in the field and the virtual machine it will use that YAML metadata file to expand the the sensor data back to the full definition so that it knows what BigQuery table should go in, should the data go in, to the bigquery table And and then i level the bigquery table and there are different different cloud functions that are scheduled at different times um some are pulling weather data from different locations um some are pulling forecast data some are like automatically using that data to process and compute evapotranspiration to compute crop water stress index to compute soil water stress index um and all these cloud functions just get triggered automatically and then they automatically just compute all these things from google cloud from Google Cloud. All right. So this is kind of the summary of the whole thing. And the whole idea is this is all happening in mere real time, right? Like, ideally, every 30 minutes, like, your data in Google Cloud is being updated. And all the processing you need to do is happening on the cloud functions with the frequency that you need it to happen. You know, so that's the idea.
</overall_system_description>


        </content>
    </file>
    <file>
        <name>system_diagrams.xml</name>
        <path>system_diagrams.xml</path>
        <content>
<diagram1>
Filename: crop_to_cloud_data_flow_part1.png
Name: Crop to Cloud Data Flow - Part 1
Description: Illustrates the journey of data from field sensors to the MQTT broker, showcasing the initial stages of data collection and transmission in the IoT system.
Code:
sequenceDiagram
    participant Sensors as Field Sensors
    participant Logger as Campbell CR800 Logger
    participant RPi as Raspberry Pi
    participant LoRaHAT as LoRa HAT
    participant Gateway as LoRa Gateway
    participant EMQX as EMQX MQTT Broker

    Sensors->>Logger: Send sensor data
    Logger->>RPi: Transfer raw data 
    (every 30 min)
    RPi->>RPi: Process and format data
    RPi->>RPi: Convert to hashes
    RPi->>LoRaHAT: Send formatted data
    LoRaHAT->>Gateway: Transmit data packet
    Gateway->>EMQX: Publish to MQTT topic
    Note over EMQX: Data ready 
    for cloud processing
</diagram1>

<diagram2>
Filename: crop_to_cloud_data_flow_part2.png
Name: Crop to Cloud Data Flow - Part 2
Description: Depicts the cloud-based processing of irrigation data, from MQTT broker to BigQuery storage, including PubSub messaging and Cloud Function computations.
Code:
sequenceDiagram
    participant EMQX as EMQX MQTT Broker
    participant VM as Virtual Machine Forwarder
    participant PubSub as Google Cloud PubSub
    participant CF as Cloud Function
    participant BQ as BigQuery
    participant Scheduler as Cloud Scheduler

    EMQX->>VM: Forward MQTT message
    VM->>VM: Process message
    VM->>PubSub: Publish to PubSub topic
    PubSub->>CF: Trigger Cloud Function
    CF->>CF: Process data
    CF->>BQ: Insert into BigQuery table
    Scheduler->>CF: Trigger periodic functions
    Note over CF: CWSI, ET_ref, 
    SWSI computations
    CF->>BQ: Update computed metrics
</diagram2>

<diagram3>
Filename: cloud_functions_workflow.png
Name: Cloud Functions Workflow
Description: Provides an overview of various cloud functions, including weather data retrieval, forecasting, and irrigation metric computations, showing how these functions interact with BigQuery and are triggered by Cloud Scheduler.
Code:
graph TD
    A[Cloud Scheduler] -->|Triggers| B(Current Weather Function)
    A -->|Triggers| C(Forecast Function)
    A -->|Triggers| D(CWSI Computation)
    A -->|Triggers| E(ET_ref Calculation)
    A -->|Triggers| F(SWSI Computation)
    B -->|Updates| G[BigQuery Tables]
    C -->|Updates| G
    D -->|Updates| G
    E -->|Updates| G
    F -->|Updates| G
    H[PubSub Messages] -->|Trigger| I(Data Insertion Function)
    I -->|Inserts Data| G
</diagram3>

<diagram4>
Filename: complete_sensor_to_cloud_workflow.png
Name: Complete Sensor-to-Cloud Data Flow
Description: The image shows a left-to-right flow diagram of a data collection and processing system. It consists of several components connected by red arrows.

Starting from the left:

1. There are three small images labeled "SENSORS (ORIGIN)":
   - A close-up of a plant stem labeled "STEM DIAMETER"
   - A device pointing upward labeled "CANOPY TEMPERATURE"
   - Yellow cables in soil labeled "SOIL MOISTURE"

2. A red arrow points from these to the next major component:
   - A large image of an open electrical box with various components inside
   - Labels point to different parts:
     - "CAMPBELL DATALOGGER" at the top
     - "LORA ANTENNA" also at the top
     - "RBPI" (likely Raspberry Pi) in the middle
     - "LORA HAT" below the RBPI
     - "CHARGE CONTROLLER" to the right
     - "12V BATTERY" at the bottom
     - "POWER REGULATOR" also at the bottom

3. Another red arrow points to the next item:
   - A white, rectangular device labeled "LORAWAN GATEWAY"

4. The next red arrow points to:
   - A purple logo with white text reading "MQTT Broker"

5. Below this, connected by a red arrow, is:
   - A blue square with an icon of a computer screen, labeled "VM" and "MQTT PACKET FOWARDED" (likely a typo for "FORWARDER")

6. To the right, connected by a red arrow:
   - A blue hexagon with white dots and lines, labeled "GOOGLE PUB/SUB"

7. Next to this, connected by a double-headed arrow:
   - Another blue hexagon with brackets, labeled "GOOGLE CLOUD FUNCTIONS"

8. A final red arrow points to:
   - A screenshot of a spreadsheet-like interface labeled "GOOGLE BIGQUERY (DESTINATION)"

The overall layout suggests a flow of data from sensors on the left, through various processing and transmission stages, to final storage and analysis on the right.
</diagram4>


<diagram5>
Filename: agricultural_iot_dashboard.png
Name: Agricultural IoT Analytics Dashboard
Description: The image shows a web-based dashboard interface designed for agricultural data visualization and analysis. The dashboard is divided into several distinct sections:

1. Header:
   - Top left: A logo showing a stylized leaf and the text "AgriSense IoT"
   - Top right: User profile icon, notifications bell, and settings gear icon

2. Left Sidebar:
   - Navigation menu with icons and labels for: Home, Fields, Sensors, Analytics, Alerts, and Settings

3. Main Content Area (divided into a 2x2 grid of widgets):
   Top Left Widget - "Field Overview":
   - A satellite map view of agricultural fields
   - Fields are color-coded based on soil moisture levels
   - A legend shows colors ranging from red (dry) to blue (wet)

   Top Right Widget - "Sensor Readings":
   - Three gauges showing current readings for:
     1. Average Soil Moisture: 32%
     2. Average Canopy Temperature: 28°C
     3. Average Stem Diameter: 15mm
   - Each gauge has a needle pointing to the current value and is color-coded (green, yellow, or red) to indicate status

   Bottom Left Widget - "7-Day Trend":
   - A line graph showing the last 7 days of data
   - Y-axis: Values for soil moisture, temperature, and stem diameter
   - X-axis: Dates
   - Three lines in different colors represent each measurement type

   Bottom Right Widget - "Alerts and Recommendations":
   - A scrollable list of recent alerts and AI-generated recommendations, such as:
     1. "Field 3 soil moisture below threshold - Consider irrigation"
     2. "Predicted heat stress in Field 2 - Increase monitoring"
     3. "Optimal harvest time for Field 1 approaching - Schedule equipment"

4. Footer:
   - Links to "Help", "Contact Support", and "Privacy Policy"
   - Text showing "Last updated: [Current Date and Time]"

The overall color scheme uses shades of green and blue, evoking an agricultural and technological feel. The layout is clean and modern, with clear data visualizations and actionable insights prominently displayed.
</diagram5>

        </content>
    </file>
</directory>
</repository_structure>
