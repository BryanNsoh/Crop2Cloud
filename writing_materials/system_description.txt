<overall_system_description>
here's the plan so I have a research project that I'm in the process of converting to a research paper. Here is the overall context. It is it is a system for automated data collection and processing for agricultural monitoring for irrigation scheduling. So I'm going to begin from the source of the data. There are I'm gonna begin from the source of the data. There are TDR sensors, that is T D R, placed at three different depths within the soil, six inches. depths within the soil. 6 inches, 18 inches and 32 inches each. 6 inches apart there are canopy temperature sensors raised one meter above the canopy and placed at a 45 degree angle pointing towards the crop and then there are these four sensors. Dendrometers placed around the plant stem. So there are these four sensors. The TDR, that is TDR and IRT, infrared temperature sensors. TDR stands for time reflectometry sensors. Those use the SDI-12 communication protocol. Meanwhile, the dendrometer is an analog sensor. So, each of these are connected to a data logger, CR800 data logger from Campbell Scientific. The data logger will sample the data every five minutes and then average that data over the course of 30 minutes and store the data every 30 minutes. Then a Raspberry Pi is plugged into the data logger via an RS-232 to USB adapter which allows the Raspberry Pi to talk with the Campbell scientific data logger and interface with it using the PI Campbell 1000 PI Campbell CR 1000 sorry library which allows the communication So the Raspberry Pi itself and then it will also send that data using a LoRa module attached to its pins right so it's called a LoRa hat from Pi supply and it basically turns the Raspberry Pi Raspberry Pi 4 into a LoRa node one challenge is that the specific nomenclature that is used to identify each sensor in the field and you will see this nomenclature in the sensor data YAML. This specific nomenclature helps one identify the sensor and immediately tells you all sensor metadata but then it's too much to transfer over LoRa because the bigger the packet size the more power it takes to send and the fewer sensor data you can send together so one needs to be able to at the same time easily identify sensors and to be able to send sensors sensor data over the LoRa network with a small header. So a sensor name to hash mapping is created so that the sensors can be sent only with a three digitdigit hash that is sufficient to encode all sensor data sent over the network. So the data is gotten from the logger. Each header is assigned a hash, a three-digit hash that maps it to all the required metadata for that sensor. And the metadata includes the sensor name as well as all the Google BigQuery credentials that is needed for data piece of sensor data is encoded within the the mapping file which is a yaml file and with that three digit hash you can basically know everything you need to know about the sensor. So the sensor data is sent from the Raspberry Pi through the LoRa hat as simple JSON packets, right? So the JSON is encoded into base 64, and we're only using, we're sending the sensor data one packet or the data from one sensor at a time. So we have just a timestamp to identify when the data from one sensor at a time. So we have just a timestamp to identify when the data was taken. And then we have the... the actual sensor data so those two are put into a json converted to base 64 sent over LoRa then we have a gateway which is about a mile from the sensor a LoRa then we have a gateway which is about a mile from the sensor also like all the kind of LoRa settings are in the code so there's a gateway mounted where all of these sensors send their data to it. The water is so clear that I can't see the sea. I'm going to take a walk in the water. I'm going to take a walk in the water. I'm going to take a walk in the water. I'm going to take a walk in the water. send their data to the gateway and then the gateway maybe we can also go through the gateway setup but the gateways got to like fiber optic poles you know gateway receives the signals and then kind of it's also subscribed to an mqtt um broker the one we're using is EMQX so it basically takes each packet and forwards it to the EMQX MQTT broker all right and then the EMQX MQTT broker receives those packages and forwards them on. So actually here, I'm not sure about the termination. Like what exactly is EMQX in this context? But anyway, I have like another MQTT thing running on the virtual machine using Mosquito and it will receive those packets from EMQX. It's running on the virtual machine and the virtual machine is perpetually running. Receive those packets, and then it will forward the packets to Google PubSub thing, right? So the PubSub topic and really dig into the details of this. and really dig into the details of this. The PubSub topic is triggered by kind of a cloud function. So the PubSub triggers a cloud function. So when it receives like the data packets, um it will It will then write those data packets to the required BigQuery table. But then, remember, we had a hash, right? So where does that hash get kind of used and kind of converted back to the relevant data? That happens at the level of the virtual machine. So the virtual machine will see a hash and a corresponding timestamp. And then it will use the uploaded metadata file, YAML metadata file which is exists at the level of the Raspberry Pi in the field and the virtual machine it will use that YAML metadata file to expand the the sensor data back to the full definition so that it knows what BigQuery table should go in, should the data go in, to the bigquery table And and then i level the bigquery table and there are different different cloud functions that are scheduled at different times um some are pulling weather data from different locations um some are pulling forecast data some are like automatically using that data to process and compute evapotranspiration to compute crop water stress index to compute soil water stress index um and all these cloud functions just get triggered automatically and then they automatically just compute all these things from google cloud from Google Cloud. All right. So this is kind of the summary of the whole thing. And the whole idea is this is all happening in mere real time, right? Like, ideally, every 30 minutes, like, your data in Google Cloud is being updated. And all the processing you need to do is happening on the cloud functions with the frequency that you need it to happen. You know, so that's the idea.
</overall_system_description>

